# Google Data Trends Daily Insight

## Problem Definition

Destination tourism can have a significant impact on a local economy by generating revenue through tourism-related activities. When tourists visit a destination, they typically spend money on accommodations, food, transportation, and activities, which in turn creates jobs and boosts the local economy.

One important aspect of destination tourism is the multiplier effect. This refers to the additional economic activity generated by the initial spending of tourists. For example, when a tourist stays at a hotel, the hotel may purchase supplies from local businesses, such as food and cleaning supplies, creating additional economic activity.

In addition to the direct economic benefits of destination tourism, there can also be indirect benefits. Tourism can increase the visibility and reputation of a destination, which can attract additional investment and business opportunities. Additionally, tourism can help to preserve and promote local culture and heritage, which can in turn attract more tourists.

therefore here I am conducting data analysis from tourist destinations in Indonesia to see the potential for development in the region and analyze the unattractiveness of these destinations in the eyes of tourists

***

## Solution Overview

![Untitled (100 Ã— 100 cm)](https://user-images.githubusercontent.com/79616397/230957215-4daeeadb-353e-431e-95df-3243d1a9bfa1.png)

therefore here I am conducting data analysis from tourist destinations in Indonesia to see the potential for development in the region and analyze the unattractiveness of these destinations in the eyes of tourists


### Tools
Google Cloud Platform (GCP): Cloud-based auto-scaling platform by Google
Google Cloud Storage (GCS): Data Lake
BigQuery: Data Warehouse
Terraform: Infrastructure-as-Code (IaC)
Docker: Containerization
Prefect: Workflow Orchestration

***

## Step-by-step Guide

### Part 1: Infrastructure as Code Cloud Resource with Terraform

1. Install `gcloud SDK`, `terraform`, and create a GCP project. 

2. Create a service account with **Storage Admin**, **Storage Pbject Admin**, **BigQuery Admin** role. 

3. Create and Download the JSON credential and store it on `.google/credentials/google_credential.json`

4. Edit `1-terraform/main.tf` in a text editor, and change `de-1199` with GCP's project id.

5. Move directory to `1-terraform` by executing
    ```
    cd 1-terraform
    ```

6. Initialize Terraform (set up environment and install Google provider)
    ```
    terraform init
    ```
7. Plan Terraform infrastructure creation
    ```
    terraform plan
    ```
8. Create new infrastructure by applying Terraform plan
    ```
    terraform apply
    ```
9. Check GCP console to see newly-created resource `GCS Bucket`, `Big Query Dataset`, and `Virtual Machine`.


### Part 2: Run Prefect to Scrape, Ingest, and Warehouse Data
1. Go to source directory
    ```
    cd ..
    ```
Then, replace `de-1199` in `.env` file to project ID.

2. Build Prefect docker images
    ```
    docker-compose build
    ```
3. Run Prefect docker containers
    ```
    docker-compose up
    ```
or for daemonized run
    ```
    docker-compose up -d
    ```
4. Access the Prefect webserver through web browser on `localhost:4200` and create block `GCS Bucket` (with name GCS Bucket) and `GCS Credentials`(from file `google_credential.json`)
   
![Airflow Webserver UI](docs/airflow-ui.png)

5. Run Workflow `parent-workflow.py` and schedule it in every night
    ```
    prefect deployment build parent-workflow.py:etl_parent_flow -n "Parameterized ETL"
    ```
    ![image](https://user-images.githubusercontent.com/79616397/230938319-f8cab849-eb08-4fa4-8c43-86b6c89b4b73.png)
    ![image](https://user-images.githubusercontent.com/79616397/230957720-77728d87-2bcd-41cc-82d9-235a6f395852.png)

6. Run prefect agent to start queue schedule
    ```
    prefect agent start --work-queue "default" 
    ```

7. After done, check GCP Console, both in Google Cloud Storage and BigQuery to check the data.

    ![image](https://user-images.githubusercontent.com/79616397/230944184-a4f75913-d9fa-435a-b96b-913ac681b1ca.png)
    ![image](https://user-images.githubusercontent.com/79616397/230944066-989c1113-71dc-4726-927c-4f0d195d2e03.png)


### Part 3: Visualize Data
1. Open Looker Website or Tableau Desktop, and connect to BigQuery.
2. Authorize credentials service aaccount from `google_credential.json`
2. Visualize the dashboard, publish to Public.

![image](https://user-images.githubusercontent.com/79616397/230955196-088a05e8-9d5e-49ec-a67a-404e7f638df0.png)

### Part 4: Stopping Project
1. To shut down the project, just stop the docker container
```
docker-compose down
```

### Data Engineering Zoomcamp by DataTalksClub
https://github.com/DataTalksClub/data-engineering-zoomcamp
